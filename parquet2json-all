#!/usr/bin/env -S uv run --quiet --script
# /// script
# dependencies = [
#   "click",
# ]
# ///

from click import argument, command, option
from concurrent.futures import ThreadPoolExecutor, as_completed
from functools import partial
from subprocess import run, PIPE
from sys import stderr, exit
import re

err = partial(print, file=stderr)

def parquet2json(*args):
    """Run parquet-2-json.sh and return stdout"""
    result = run(['parquet-2-json.sh', *map(str, args)], capture_output=True, text=True)
    if result.returncode != 0:
        err(f"parquet-2-json.sh failed: {result.stderr}")
        exit(result.returncode)
    return result.stdout.rstrip()

def get_file_info(path):
    """Get MD5 and size of file (S3 or local)"""
    if path.startswith('s3://'):
        # Run both AWS commands concurrently
        with ThreadPoolExecutor(max_workers=2) as executor:
            md5_future = executor.submit(
                lambda: run(['bash', '-c', f'aws s3 cp "{path}" - | tee >(wc -c >&2) | md5sum | cut -d" " -f1'],
                           stdout=PIPE, stderr=PIPE, text=True)
            )

            result = md5_future.result()
            md5 = result.stdout.strip()
            size = result.stderr.strip()
    else:
        result = run(['bash', '-c', f'md5sum "{path}" | cut -d" " -f1'], capture_output=True, text=True)
        md5 = result.stdout.strip()
        result = run(['bash', '-c', f'stat -c %s "{path}"'], capture_output=True, text=True)
        size = result.stdout.strip()
    return md5, size

@command()
@option('-n', '--num-rows', default='3', help='Number of first and last rows to display; comma-separate to distinguish head/tail (e.g. default `-n3` is equivalent to `-n3,3`). `,` prints all rows.')
@option('-o', '--offset', type=int, default=0, help='Skip this number of rows; negative â‡’ skip to last N rows')
@option('-s', '--compact', is_flag=True, help='Compact mode (one object per line)')
@argument('path', required=False, default='-')
def main(num_rows, offset, compact, path):
    """Display parquet file metadata and sample rows"""

    # Parse num_rows
    if num_rows == ',':
        head_rows = 0
        tail_rows = 0
        print_all = True
    elif ',' in num_rows:
        head_str, tail_str = num_rows.split(',', 1)
        head_rows = int(head_str) if head_str else 0
        tail_rows = int(tail_str) if tail_str else 0
        print_all = False
    else:
        n = int(num_rows)
        head_rows = tail_rows = n
        print_all = False

    # Handle stdin
    if path == '-' or not path:
        import tempfile
        with tempfile.NamedTemporaryFile(delete=False) as tmp:
            import shutil
            shutil.copyfileobj(__import__('sys').stdin.buffer, tmp)
            path = tmp.name

    # Check file exists
    if not path.startswith('s3://'):
        from os.path import exists
        if not exists(path):
            err(f"File not found: {path}")
            exit(127)

    print_cmd = 'cat' if compact else 'jq'

    # Run metadata queries concurrently
    with ThreadPoolExecutor(max_workers=4) as executor:
        rowcount_future = executor.submit(parquet2json, 'rowcount', path)
        schema_future = executor.submit(parquet2json, 'schema', path)
        file_info_future = executor.submit(get_file_info, path)

        rowcount = int(rowcount_future.result())
        schema = schema_future.result()
        md5, size = file_info_future.result()

    print(f"MD5: {md5}")
    print(f"{size} bytes")
    print(f"{rowcount} rows")
    print(schema)

    offset_args = [f'-o{offset}'] if offset else []
    show_rows = head_rows + tail_rows + abs(offset)

    if print_all or show_rows >= rowcount:
        # Single query for all rows
        output = parquet2json('cat', *offset_args, path)
        if print_cmd == 'jq':
            result = run(['jq'], input=output, capture_output=True, text=True)
            print(result.stdout, end='')
        else:
            print(output)
    else:
        # Run head and tail queries concurrently
        futures = {}
        with ThreadPoolExecutor(max_workers=2) as executor:
            if head_rows > 0:
                futures['head'] = executor.submit(parquet2json, 'cat', '-l', head_rows, *offset_args, path)
            if tail_rows > 0:
                futures['tail'] = executor.submit(parquet2json, 'cat', f'-o-{tail_rows}', path)

            if 'head' in futures:
                label = "First row:" if head_rows == 1 else f"First {head_rows} rows:"
                print(label)
                output = futures['head'].result()
                if print_cmd == 'jq':
                    result = run(['jq'], input=output, capture_output=True, text=True)
                    print(result.stdout, end='')
                else:
                    print(output)

            if 'tail' in futures:
                label = "Last row:" if tail_rows == 1 else f"Last {tail_rows} rows:"
                print(label)
                output = futures['tail'].result()
                if print_cmd == 'jq':
                    result = run(['jq'], input=output, capture_output=True, text=True)
                    print(result.stdout, end='')
                else:
                    print(output)

if __name__ == '__main__':
    main()
