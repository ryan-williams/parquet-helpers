#!/usr/bin/env -S uv run --quiet --script
# /// script
# dependencies = [
#   "click",
#   "utz",
# ]
# ///

from concurrent.futures import ThreadPoolExecutor
from os import environ
from os.path import exists
from subprocess import CalledProcessError, PIPE, run
from sys import exit
from utz import err
from utz.cli import arg, cmd, flag, opt
from utz.proc import text


def parquet2json(*args):
    """Run parquet-2-json.sh and return stdout. Raises CalledProcessError on failure."""
    result = run(
        ['parquet-2-json.sh', *map(str, args)],
        capture_output=True,
        text=True,
    )
    if result.returncode != 0:
        # Combine stdout and stderr for error context
        output = (result.stdout + result.stderr).strip()
        raise CalledProcessError(result.returncode, result.args, output=output)
    return result.stdout.rstrip()


def get_file_info(path):
    """Get MD5 and size of file (S3 or local)"""
    if path.startswith('s3://'):
        # Use single aws s3 cp with tee to compute both MD5 and size from same stream
        result = run(
            ['bash', '-c', f'aws s3 cp "{path}" - | tee >(wc -c >&2) | md5sum | cut -d" " -f1'],
            stdout=PIPE, stderr=PIPE, text=True
        )
        md5 = result.stdout.strip()
        size = result.stderr.strip()
    else:
        md5 = text('bash', '-c', f'md5sum "{path}" | cut -d" " -f1', log=None).strip()
        size = text('bash', '-c', f'stat -c %s "{path}"', log=None).strip()
    return md5, size


DEFAULT_NUM_ROWS = 3
DEFAULT_OFFSET = 0


@cmd(context_settings=dict(help_option_names=['-h', '--help']))
@opt('-n', '--num-rows', help=f'Number of first and last rows to display (default: {DEFAULT_NUM_ROWS}); comma-separate to distinguish head/tail (e.g. `-n3` is equivalent to `-n3,3`). `,` prints all rows. Env var $PQT_TXT_OPTS (or ${{PQT_TXT_OPTS_VAR}}) overrides CLI options.')
@opt('-o', '--offset', type=int, help=f'Skip this number of rows (default: {DEFAULT_OFFSET}); negative â‡’ skip to last N rows')
@flag('-s', '--compact', help='Compact mode (one object per line)')
@arg('path', required=False, default='-')
def main(num_rows, offset, compact, path):
    """Display parquet file metadata and sample rows"""

    # Handle env var options (override CLI options)
    opts_var = environ.get('PQT_TXT_OPTS_VAR', 'PQT_TXT_OPTS')
    opts_str = environ.get(opts_var, '').strip()
    if opts_str:
        import shlex
        from click import Context
        env_opts = shlex.split(opts_str)
        ctx = Context(main)
        main.parse_args(ctx, env_opts)
        # Only override CLI values with env values that were explicitly set (not None)
        if ctx.params.get('num_rows') is not None:
            num_rows = ctx.params['num_rows']
        if ctx.params.get('offset') is not None:
            offset = ctx.params['offset']
        if ctx.params.get('compact'):
            compact = ctx.params['compact']

    # Apply defaults for values not set by CLI or env
    if num_rows is None:
        num_rows = str(DEFAULT_NUM_ROWS)
    if offset is None:
        offset = DEFAULT_OFFSET

    # Parse num_rows
    if num_rows == ',':
        head_rows = 0
        tail_rows = 0
        print_all = True
    elif ',' in num_rows:
        head_str, tail_str = num_rows.split(',', 1)
        head_rows = int(head_str) if head_str else 0
        tail_rows = int(tail_str) if tail_str else 0
        print_all = False
    else:
        n = int(num_rows)
        head_rows = tail_rows = n
        print_all = False

    # Handle stdin
    if path == '-' or not path:
        import tempfile
        import shutil
        import sys
        with tempfile.NamedTemporaryFile(delete=False) as tmp:
            shutil.copyfileobj(sys.stdin.buffer, tmp)
            path = tmp.name

    # Check file exists
    if not path.startswith('s3://'):
        if not exists(path):
            err(f"File not found: {path}")
            exit(127)

    print_cmd = 'cat' if compact else 'jq'

    # Run metadata queries concurrently
    with ThreadPoolExecutor(max_workers=4) as executor:
        rowcount_future = executor.submit(parquet2json, 'rowcount', path)
        schema_future = executor.submit(parquet2json, 'schema', path)
        file_info_future = executor.submit(get_file_info, path)

        try:
            rowcount = int(rowcount_future.result())
            schema = schema_future.result()
            md5, size = file_info_future.result()
        except CalledProcessError as e:
            # Extract a cleaner error message from parquet2json output
            output = e.output or ''
            if 'NotFound' in output or 'status: 404' in output:
                err(f"File not found: {path}")
            elif 'status: 403' in output:
                err(f"Access denied: {path}")
            elif 'ExpiredToken' in output or 'credentials' in output.lower():
                err(f"AWS credentials error: {path}")
            elif 'Invalid Parquet file' in output or 'magic' in output.lower():
                err(f"Invalid Parquet file: {path}")
            else:
                # Find the most useful error line (skip panic location, stack traces)
                for line in output.splitlines():
                    line = line.strip()
                    if not line:
                        continue
                    # Skip rust panic header and stack trace lines
                    if line.startswith(("thread '", 'stack backtrace', 'note:', '   ', '  ')):
                        continue
                    err(line)
                    break
                else:
                    err(f"Error reading: {path}")
            exit(e.returncode)

    print(f"MD5: {md5}")
    print(f"{size} bytes")
    print(f"{rowcount} rows")
    print(schema)

    offset_args = [f'-o{offset}'] if offset else []
    show_rows = head_rows + tail_rows + abs(offset)

    if print_all or show_rows >= rowcount:
        # Single query for all rows
        output = parquet2json('cat', *offset_args, path)
        if print_cmd == 'jq':
            result = run(['jq'], input=output, capture_output=True, text=True)
            print(result.stdout, end='')
        else:
            print(output)
    else:
        # Run head and tail queries concurrently
        futures = {}
        with ThreadPoolExecutor(max_workers=2) as executor:
            if head_rows > 0:
                futures['head'] = executor.submit(parquet2json, 'cat', '-l', head_rows, *offset_args, path)
            if tail_rows > 0:
                futures['tail'] = executor.submit(parquet2json, 'cat', f'-o-{tail_rows}', path)

            if 'head' in futures:
                label = "First row:" if head_rows == 1 else f"First {head_rows} rows:"
                print(label)
                output = futures['head'].result()
                if print_cmd == 'jq':
                    result = run(['jq'], input=output, capture_output=True, text=True)
                    print(result.stdout, end='')
                else:
                    print(output)

            if 'tail' in futures:
                label = "Last row:" if tail_rows == 1 else f"Last {tail_rows} rows:"
                print(label)
                output = futures['tail'].result()
                if print_cmd == 'jq':
                    result = run(['jq'], input=output, capture_output=True, text=True)
                    print(result.stdout, end='')
                else:
                    print(output)


if __name__ == '__main__':
    main()
