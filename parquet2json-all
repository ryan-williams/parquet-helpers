#!/usr/bin/env -S uv run --quiet --script
# /// script
# dependencies = [
#   "click",
#   "utz",
# ]
# ///

from concurrent.futures import ThreadPoolExecutor
from os import environ
from os.path import exists
from subprocess import PIPE, run
from sys import exit
from utz import err
from utz.cli import arg, cmd, flag, opt
from utz.proc import text


def parquet2json(*args):
    """Run parquet-2-json.sh and return stdout"""
    return text('parquet-2-json.sh', *map(str, args), log=None).rstrip()


def get_file_info(path):
    """Get MD5 and size of file (S3 or local)"""
    if path.startswith('s3://'):
        # Use single aws s3 cp with tee to compute both MD5 and size from same stream
        result = run(
            ['bash', '-c', f'aws s3 cp "{path}" - | tee >(wc -c >&2) | md5sum | cut -d" " -f1'],
            stdout=PIPE, stderr=PIPE, text=True
        )
        md5 = result.stdout.strip()
        size = result.stderr.strip()
    else:
        md5 = text('bash', '-c', f'md5sum "{path}" | cut -d" " -f1', log=None).strip()
        size = text('bash', '-c', f'stat -c %s "{path}"', log=None).strip()
    return md5, size


@cmd
@opt('-n', '--num-rows', default='3', help='Number of first and last rows to display; comma-separate to distinguish head/tail (e.g. default `-n3` is equivalent to `-n3,3`). `,` prints all rows. Env var $PQT_TXT_OPTS (or ${PQT_TXT_OPTS_VAR}) overrides CLI options.')
@opt('-o', '--offset', type=int, default=0, help='Skip this number of rows; negative â‡’ skip to last N rows')
@flag('-s', '--compact', help='Compact mode (one object per line)')
@arg('path', required=False, default='-')
def main(num_rows, offset, compact, path):
    """Display parquet file metadata and sample rows"""

    # Handle env var options (override CLI options)
    opts_var = environ.get('PQT_TXT_OPTS_VAR', 'PQT_TXT_OPTS')
    opts_str = environ.get(opts_var, '').strip()
    if opts_str:
        import shlex
        env_opts = shlex.split(opts_str)
        # Parse env options to override CLI values
        i = 0
        while i < len(env_opts):
            opt = env_opts[i]
            if opt.startswith('-n'):
                if opt == '-n' and i + 1 < len(env_opts):
                    num_rows = env_opts[i + 1]
                    i += 2
                else:
                    num_rows = opt[2:]  # -n3 format
                    i += 1
            elif opt.startswith('-o'):
                if opt == '-o' and i + 1 < len(env_opts):
                    offset = int(env_opts[i + 1])
                    i += 2
                else:
                    offset = int(opt[2:])  # -o5 format
                    i += 1
            elif opt == '-s' or opt == '--compact':
                compact = True
                i += 1
            else:
                i += 1

    # Parse num_rows
    if num_rows == ',':
        head_rows = 0
        tail_rows = 0
        print_all = True
    elif ',' in num_rows:
        head_str, tail_str = num_rows.split(',', 1)
        head_rows = int(head_str) if head_str else 0
        tail_rows = int(tail_str) if tail_str else 0
        print_all = False
    else:
        n = int(num_rows)
        head_rows = tail_rows = n
        print_all = False

    # Handle stdin
    if path == '-' or not path:
        import tempfile
        import shutil
        import sys
        with tempfile.NamedTemporaryFile(delete=False) as tmp:
            shutil.copyfileobj(sys.stdin.buffer, tmp)
            path = tmp.name

    # Check file exists
    if not path.startswith('s3://'):
        if not exists(path):
            err(f"File not found: {path}")
            exit(127)

    print_cmd = 'cat' if compact else 'jq'

    # Run metadata queries concurrently
    with ThreadPoolExecutor(max_workers=4) as executor:
        rowcount_future = executor.submit(parquet2json, 'rowcount', path)
        schema_future = executor.submit(parquet2json, 'schema', path)
        file_info_future = executor.submit(get_file_info, path)

        rowcount = int(rowcount_future.result())
        schema = schema_future.result()
        md5, size = file_info_future.result()

    print(f"MD5: {md5}")
    print(f"{size} bytes")
    print(f"{rowcount} rows")
    print(schema)

    offset_args = [f'-o{offset}'] if offset else []
    show_rows = head_rows + tail_rows + abs(offset)

    if print_all or show_rows >= rowcount:
        # Single query for all rows
        output = parquet2json('cat', *offset_args, path)
        if print_cmd == 'jq':
            result = run(['jq'], input=output, capture_output=True, text=True)
            print(result.stdout, end='')
        else:
            print(output)
    else:
        # Run head and tail queries concurrently
        futures = {}
        with ThreadPoolExecutor(max_workers=2) as executor:
            if head_rows > 0:
                futures['head'] = executor.submit(parquet2json, 'cat', '-l', head_rows, *offset_args, path)
            if tail_rows > 0:
                futures['tail'] = executor.submit(parquet2json, 'cat', f'-o-{tail_rows}', path)

            if 'head' in futures:
                label = "First row:" if head_rows == 1 else f"First {head_rows} rows:"
                print(label)
                output = futures['head'].result()
                if print_cmd == 'jq':
                    result = run(['jq'], input=output, capture_output=True, text=True)
                    print(result.stdout, end='')
                else:
                    print(output)

            if 'tail' in futures:
                label = "Last row:" if tail_rows == 1 else f"Last {tail_rows} rows:"
                print(label)
                output = futures['tail'].result()
                if print_cmd == 'jq':
                    result = run(['jq'], input=output, capture_output=True, text=True)
                    print(result.stdout, end='')
                else:
                    print(output)


if __name__ == '__main__':
    main()
